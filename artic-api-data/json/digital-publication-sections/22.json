{
    "id": 22,
    "api_model": "digital-publication-sections",
    "api_link": "https://api.artic.edu/api/v1/digital-publication-sections/22",
    "title": "Crowdsourcing Metadata in Museums: Expanding Descriptions, Access, Transparency, and Experience",
    "web_url": "https://nocache.www.artic.edu/digital-publications/37/perspectives-on-data/22/crowdsourcing-metadata-in-museums-expanding-descriptions-access-transparency-and-experience",
    "copy": " Museums have experienced a technological boom over the last fifty years, and digital access to collections has evolved from searchable catalogues available only on-site to a variety of modalities ranging from web-based, publicly available databases to interaction through social media platforms. As museums look to capitalize on the new ways in which their collections are being discovered, cataloguing visual data and expanding metadata are necessary for staying relevant, on trend, and engaged with audiences.[ref]Brendan Ciecko, Hilary-Morgan Watt, and Emily Haight, \u201cHow Museums Can Experiment with Social Media to Boost Audience Engagement During Coronavirus,\u201d April 1, 2020, Cuseum, webinar, 58:02, https://cuseum.com/webinars/how-museums-can-experiment-with-social-media-to-boost-audience-engagement-during-coronavirus-overview .[/ref] As usually defined, metadata is a set of data that provides information about other data.[ref]Steven Miller, Metadata for Digital Collections (New York: Neal-Schuman Publishers, 2011), 179.[/ref] A piece of metadata typically consists of a set of properties (elements or fields) and a set of values for each property: for example, Title Field: \u201cThe Mona Lisa,\u201d Accession Number: \u201c2010.030.0001,\u201d and so on. Metadata allows people to perform various operations with data, including searching, managing, structuring, preserving, and authenticating resources. Creating metadata can be labor intensive, and one solution to the need for more extensive cataloguing is crowdsourcing, which over the last two decades has proven not only to increase access points to collections but also to enrich catalogue data.[ref]Jennifer Trant, \u201cTagging, Folksonomy and Art Museums: Results of steve.museum\u2019s Research,\u201d University of Arizona, Digital Library of Information Science & Technology, January 2009, http://hdl.handle.net/10150/105627 .[/ref] As well, crowdsourcing presents an opportunity for museums to make what has long been an opaque back-end process more transparent, turning metadata creation into a mission-supporting activity.[ref]Mary Flanagan et al., \u201cCitizen Archivists at Play: Game Design for Gathering Metadata for Cultural Heritage Institutions,\u201d Proceedings of DiGRA 2013: DeFragging Game Studies, August 2014, http://www.digra.org/digital-library/publications/citizen-archivists-at-play-game-design-for-gathering-metadata-for-cultural-heritage-institutions/ .[/ref] As Meghan Ferriter, Samantha Blickhan, and Mia Ridge, the leaders of the Collective Wisdom project, state, at their best, crowdsourcing projects from cultural institutions create \u201cmeaningful opportunities for the public to experience collections while undertaking a range of tasks that make those collections more easily discoverable.\u201d[ref]\u201cAbout,\u201d Collective Wisdom : The State of the Art in Crowdsourcing in Cultural Heritage, https://collectivewisdomproject.org.uk/about/ .[/ref] Using an adapted practice-based methodology, this article takes a project I devised and led at Chicago\u2019s Adler Planetarium, Tag Along with Adler , as a case study in the benefits of crowdsourcing projects (and metadata tagging projects in particular) within museums, not as mere outsourcing of labor but rather as participatory, even transformational experiences for an engaged public that also enhance and expand cataloguing.[ref]Laura Carletti et al., \u201cDigital Humanities and Crowdsourcing: An Exploration,\u201d Museums and the Web 2013, April 17 \u201320, 2013, https://mw2013.museumsandtheweb.com/paper/digital-humanities-and-crowdsourcing-an-exploration-4/ .[/ref] It also explores the successes and shortcomings of this ongoing project and what these early results suggest for the field at large with respect to language and metadata production. In particular, it demonstrates that there exists a semantic gap in the language and descriptive styles of museum professionals, on the one hand, and the public, on the other, and that crowdsourcing demonstrates promise to help bridge this gap while also providing an opportunity for the public to engage with museums directly. The Development of Tag Along with Adler The Adler Planetarium\u2019s Tag Along with Adler project is an ideal case study because of the varied nature of the Adler\u2019s collection, which encompasses archival (100,000+ linear feet), library (6,000+ books), and object (3,000+ historic artifacts) holdings, and because the project uses Zooniverse, the world\u2019s largest and most-used platform for citizen science, with over two million registered users globally.[ref]Citizen science involves the collaboration of the general public with professional scientists to conduct research. The principal benefit of this method is that it enables research that would not otherwise be possible, but it can also provide the opportunity to engage a more diverse audience that may include typically underrepresented skills or demographics. See Helen Spiers et al., \u201cEveryone Counts?,\u201d Journal of Science Communication 18, no. 1 (January 2019), https://jcom.sissa.it/archive/18/01/JCOM_1801_2019_A04 .[/ref] Zooniverse began in 2007 as a single astrophysics citizen-science project, Galaxy Zoo , which was a collaboration between the Adler Planetarium and Oxford University.[ref]Chris Lintott et al., \u201cGalaxy Zoo: Morphologies Derived from Visual Inspection of Galaxies from the Sloan Digital Sky Survey,\u201d Monthly Notices of the Royal Astronomical Society 389, no. 3: 1179\u201389, https://doi.org/10.1111/j.1365-2966.2008.13689.x .[/ref] Since Galaxy Zoo, Zooniverse has grown into an online platform hosting humanities- and collections-based crowdsourcing projects, and has also expanded to include mobile app capabilities.[ref]Samantha Blickhan et al., \u201cIndividual vs. Collaborative Methods of Crowdsourced Transcription,\u201d in \u201cCollecting, Preserving, and Disseminating Endangered Cultural Heritage for New Understandings through Multilingual Approaches,\u201d special issue, Journal of Data Mining and Digital Humanities (December 2019), https://hal.archives-ouvertes.fr/hal-02280013 .[/ref] Moreover, the Adler\u2019s audience has already been introduced to the Zooniverse platform through the Adler\u2019s marketing and social media, as well as on-site exhibition interactives.[ref]The ongoing Tag Along with Adler project includes various workflows and project designs to test for optimal diversity in tag generation, user motivation, and engagement with the project, but for the purposes of this paper I will focus on the Zooniverse-hosted online workflows only. As defined by Jennifer Trant, tagging is a process with the focus on user choice of terminology; an individual tag can be a word or phrase chosen by a user. Jennifer Trant, \u201cStudying Social Tagging and Folksonomy: A Review and Framework,\u201d Journal of Digital Information 10, no. 1 (January 2009), 43.[/ref] The Adler collections selected for Tag Along with Adler were chosen based on a need to present the public with collection objects they could describe relatively easily; for this reason, the project focused on two-dimensional, pictorial works. Professional cataloguing often centers on what something is (its materiality, date, creator, location, etc.) but not on what it may be about or might reflect. The distinction is not a mere matter of semantics. While the descriptive text on a wall label in a gallery, for example, might address an object\u2019s subject matter\u2014its \u201caboutness\u201d\u2014this dimension of the objects has not usually been represented in catalogues; that is, these objects and their images have not traditionally been catalogued according to their \u201caboutness . \u201d[ref]Alyx Rossetti, \u201cSubject Access and ARTstor: Preliminary Research and Recommendations for the Development of an Expert Tagging Program,\u201d Art Documentation: Journal of the Art Libraries Society of North America 32, no. 2 (Fall 2013): 284\u2013300.[/ref] Instead, museum records often focus on the facts of an object\u2019s creation, such as who made it, what it is made of, what size it is, when it was made, and where it was made. Although useful, this information does not help a person find what they\u2019re looking for when they query a database seeking images that feature a specific visual element such as children or snow. Museum educator Kris Wetterlund provides a clear example of how the difference between technical descriptive metadata and everyday language can hide materials from the wider audiences who may be searching for them\u2014in other words, how prioritizing what objects are can prevent the discovery of what they\u2019re about . As Wetterlund writes, \u201cCurators in art museums describe the medium of works of art very specifically. An oil painting in an art museum is often catalogued as oil on canvas and a black-and-white photograph is called a silver gelatin print. Thus, when teachers search museum sites [using words like] paintings or photographs , no items are returned even though art museums obviously are filled with paintings or fine art photographs.\u201d[ref]Kris Wetterlund, \u201cFlipping the Field Trip: Bringing the Art Museum to the Classroom,\u201d Theory Into Practice 47, no. 2 (Spring 2008): 110\u201317.[/ref] There is also evidence that users struggle to find materials when catalogues use only minimally descriptive metadata (a core set of metadata fields the EMDaWG [Embedded Metadata Working Group of the Smithsonian Institution] has designated essential for all collections to provide better online access to images and ensure preservation of these images in the future).[ref]Stephanie Ogeneski Christensen et al., \u201cBasic Guidelines for Minimal Descriptive Embedded Metadata in Digital Images,\u201d Embedded Metadata Working Group, Smithsonian Institution, April 2010, http://www.digitizationguidelines.gov/guidelines/GuidelinesEmbeddedMetadata.pdf .[/ref] As Jennifer Schaffner of the research division of OCLC (the Online Computer Library Center) stated in 2009: Structured metadata can be useful internally for collection management and public services, but is not always what users need most to discover primary sources, especially minimally described collections and \u201chidden collections.\u201d We understand archival standards for description and cataloging, but our users by and large don\u2019t. Studies show that users often do not want to search for collections by provenance, for example, as important as this principle is for archival collections. [ref]Jennifer Schaffner, \u201cThe Metadata Is the Interface: Better Description for Better Discovery of Archives and Special Collections, Synthesized from User Studies,\u201d OCLC: A Publication of OCLC Research, May 2009, https://library.oclc.org/digital/collection/p267701coll27/id/444/ . The term \u201chidden collections\u201d describes collections in institutions that could deepen public understanding of the histories of people of color and other communities whose work and experiences have been insufficiently recognized by traditional narratives. The term originates in a project initiated by CLIR, Digitizing Hidden Collections: Amplifying Unheard Voices , https://www.clir.org/hiddencollections/ . The term \u201cstructured metadata\u201d (also \u201cstructural metadata\u201d) is defined by the CLIR as follows: \u201cmetadata that describes the types, versions, relationships and other characteristics of digital materials.\u201d See William Arms, Christophe Blanchi, and Edward Overly, \u201cAn Architecture for Information in Digital Libraries,\u201d D-Lib Magazine , February 1997, https://www.dlib.org/dlib/february97/cnri/02arms1.html .[/ref] Schaffner argues that thirty years of user studies show that aboutness and relevance matter most for public discovery, especially when that discovery is happening online instead of on-site.[ref]Ibid.[/ref] With this problem in mind, I selected 1,090 objects from the Adler Planetarium\u2019s collection specifically because they were two-dimensional and pictorial; early results from similar projects like s teve.museum and Your Paintings Tagger found such objects easiest for users to work with.[ref]Michael Hancher, \u201cSeeing and Tagging Things in Pictures,\u201d Representations 155, no. 1 (Summer 2021): 82\u2013109, https://doi.org/10.1525/rep.2021.155.4.82 .[/ref] The group included 613 works on paper, 195 archival photographs, and 282 rare book illustrations, which were then used consistently across different tasks given to users to test how variations in the tasks users were asked to participate in (verification vs. free tagging), in the audiences who participated (Adler audience vs. Zooniverse audience), and in the platforms on which they participated (Zooniverse online, Zooniverse MuseumMode on-site, and gamified workflow) affected user engagement and the tags created. The project team began by pulling all Adler catalogue data for each of the 1,090 objects, logging only publicly searchable text in order to focus on the terms that currently facilitated or hindered public inquiry. Artificial intelligence and machine learning is \u201ca method of data analysis that automates analytical model building. Using algorithms that iteratively learn from data, machine learning allows computers to find hidden insights without being explicitly programmed where to look.\u201d[ref]Brendan Ciecko, \u201cExamining the Impact of Artificial Intelligence in Museums \u2013 MW17: Museums and the Web 2017.\u201d Accessed February 24, 2020. https://mw17.mwconf.org/paper/exploring-artificial-intelligence-in-museums/ .[/ref] In the case of this study, two separate AI tagging models were used on the selected image sets, providing two examples of computer algorithms trained to provide metadata tags on images.[ref]The two tagging models selected for this project were the image attribute classifier trained on the Metropolitan Museum of Art\u2019s iMet Collection 2019 dataset ( https://tfhub.dev/metmuseum/vision/classifier/imet_attributes_V1/1 ) and the Google Cloud Vision API ( https://cloud.google.com/vision ).[/ref] The inclusion of AI tags in this project accords with recent museum projects from the last five to ten years. Several institutions\u2014including the Barnes Foundation, Philadelphia; the Harvard Art Museums, Cambridge; the Massachusetts Institute of Technology, Cambridge; the Metropolitan Museum of Art, New York; and the Philadelphia Museum of Art\u2014have employed machine vision to analyze, categorize, and interpret their collections\u2019 images, and although such application of AI is still in its infancy, the reported results show promise.[ref] For the Metropolitan Museum of Art, see Chenyang Zhang et al., \u201cThe iMet Collection 2019 Challenge Dataset,\u201d arXiv.org, June 3, 2019, http://arxiv.org/abs/1906.00901 . For the Barnes Foundation, see Shelley Bernstein, \u201cUsing Computer Vision to Tag the Collection,\u201d Medium, October 26, 2017, https://medium.com/barnes-foundation/using-computer-vision-to-tag-the-collection-f467c4541034 . For the Massachusetts Institute of Technology, see Maria Kessler, \u201cThe Met \u00d7 Microsoft \u00d7 MIT: A Closer Look at the Collaboration,\u201d Metropolitan Museum of Art, February 21, 2019, video, 2:40, https://www.metmuseum.org/blogs/now-at-the-met/2019/met-microsoft-mit-reveal-event-video . For the Philadelphia Museum of Art, see Penn Engineering, \u201cPenn Engineering and the Philadelphia Museum of Art Join Forces to Envision the Future,\u201d Medium, November 12, 2019, https://medium.com/penn-engineering/penn-engineering-and-the-philadelphia-museum-of-art-join-forces-to-envision-the-future-bde4cbfc282f . For the Harvard Art Museums, see Harvard Art Museums, \u201cAI Explorer,\u201d https://ai.harvardartmuseums.org/ . For comprehensive results of such programs, see Brendan Ciecko, \u201cAI Sees What? The Good, the Bad, and the Ugly of Machine Vision for Museum Collections,\u201d Museums and the Web 2020, March 31\u2013April 4, 2020, https://mw20.museweb.net/paper/ai-sees-what-the-good-the-bad-and-the-ugly-of-machine-vision-for-museum-collections/ .[/ref] AI already underlies many routine aspects of our lives, and part of the motivation to include AI tags in this project was to explicitly call out the ways in which these tags are instrumental to all our daily search and discovery tasks, often in ways we do not realize.[ref]Brendan Ciecko, \u201cAI Sees What? The Good, the Bad, and the Ugly of Machine Vision for Museum Collections,\u201d Museums and the Web 2020, March 31\u2013April 4, 2020, https://mw20.museweb.net/paper/ai-sees-what-the-good-the-bad-and-the-ugly-of-machine-vision-for-museum-collections/ .[/ref] Machine vision and AI tagging are now advanced enough to detect subject matter depicted in paintings and photographs.[ref]Ibid.[/ref] Several institutions have already used AI to expand and enrich existing metadata tags, including as part of the Your Paintings Tagger project, a collaboration between the Public Catalogue Foundation and the BBC launched in 2011 to tag paintings with searchable terms.[ref]\u201cYour Paintings Project,\u201d culture ant, https://culture-ant.com/your-paintings-project/ , accessed February 28, 2022.[/ref] When volunteer taggers participating in the project were taking longer than anticipated to complete the tagging, the project team turned to automation to speed up the process.[ref]Andrew Greg, quotes in Hancher, \u201cSeeing and Tagging,\u201d 82\u2013109.[/ref] The application of machine learning to museum collections has long been subject to doubts about its accuracy and utility. In the words of Cuseum founder and CEO Brendan Ciecko, \u201cJust how well does machine vision do? Can it offer accurate tags? Is the metadata generated useful and correct?\u201d[ref]Ciecko, \u201cAI Sees What?\u201d[/ref] One scientific study found that \u201calthough computer-based analysis can address many research questions, it is yet to surpass human ability in a number of areas.\u201d[ref]Spiers et al., \u201cEveryone Counts?\u201d See also Zooniverse Help, \u201cHow to Create a Project with Our Project Builder,\u201d Zooniverse, https://help.zooniverse.org/getting-started/?_ga=2.172897696.2010710078.1613746401-631436202.1612287942 .[/ref] Not only are AI models limited in their ability to process complexity but they are also still trained by humans based on the cataloguing practices of museums. Thus these models can create exemplary tags for religious, Christian, and Western canonical images, for example, but often run into difficulties creating tags for anything \u201cother.\u201d[ref]Ciecko, \u201cAI Sees What?\u201d[/ref] For Tag Along with Adler , it is critically important to recognize that utilizing a machine removes neither bias nor semantic gaps, because these are in fact things the machines are inevitably trained in . The Adler team opted to use two AI tagging models for Tag Along with Adler : the iMet Collection Attribute Classifier and Google Cloud Vision API. The team selected them because they have been trained using more images than the Adler has access to, and both are publicly available for use by any institution. They also offer two different tagging models: one specifically trained for a museum image-based collection (the iMet Collection 2019) and one similar to the algorithms routinely encountered by users online using Google Image Search (Google Cloud Vision API). Ultimately, the Adler team included AI tags to expose project participants to this emerging technology and conduct a survey to determine whether the presence of AI in the project had motivated them to participate; it was not done to replace the participatory component of human tagging, as in the Your Paintings Tagger project. Once the tagging models generated the AI tags, the Adler created its project using the Zooniverse Project Builder.[ref]Zooniverse Help, \u201cHow to Create a Project.\u201d See also Laura Trouille et al., \u201cDIY Zooniverse Citizen Science Project: Engaging the Public with Your Museum\u2019s Collections and Data,\u201d Museums and the Web 2017, April 19\u201322, 2017, https://mw17.mwconf.org/paper/diy-your-own-zooniverse-citizen-science-project-engaging-the-public-with-your-museums-collections-and-data/ .[/ref] Although the Project Builder does not permit customization, it is a free tool that is relatively easy to use for people with little coding knowledge. Thus, the team decided to use it to make the case study replicable by any museum regardless of its budget or technical prowess. Tag Along with Adler launched on Zooniverse on March 23, 2021. The \u201cGet Started\u201d section of the homepage greeted users with descriptions of the two different workflows available to Zooniverse volunteers.[ref]\u201cHome,\u201d Tag Along with Adler , Zooniverse, https://www.zooniverse.org/projects/webster-institute/tag-along-with-adler .[/ref] The \u201cVerify AI Tags\u201d workflow aimed first and foremost at gauging the accuracy of AI tagging models and introducing the public to the positive and negative effects these models can have in their everyday lives. The second, though not subsidiary, workflow, \u201cTag Images,\u201d focused on user-generated language and gathering a diversity of opinions and perspectives. As the Adler team developed the project\u2019s descriptive text for the Zooniverse project page, we focused on how it diverges from other Zooniverse projects\u2014that is, that we were not looking for consensus or a single \u201cright\u201d answer. For many Zooniverse volunteers, this would prove to be the most challenging aspect of the project. Evaluating Tag Along with Adler The project was completed on March 12, 2022, approximately a year after it began. Tag Along with Adler used eleven subject sets of images, and as each subject set was retired, the team processed the data from both workflows (verification and textual), ultimately evaluating the results for all 1,090 images.[ref]The project was designed with incremental releases of subject sets, or groups of images. That is, instead of releasing all 1,090 images at once, the images were broken into 10 sets of 100 images, and 1 set of 90 images. The sets were uploaded and made available to the public one at a time.[/ref] In this section, I examine the ways in which this data supports adopting the citizen-science method of crowdsourcing metadata tags to demonstrate\u2014and address\u2014the semantic gaps among the language of professional cataloguers, AI algorithms, and general public users. I also argue that bridging these gaps can comprise one part of a participatory, mission-driven experience of a cultural institution. Over the twelve months of Tag Along with Adler , the project amassed 3,557 registered volunteers with 6,976 individual participants.[ref]Zooniverse projects do not require participants to register to participate, but any unregistered user who participates is assigned a single-use ID number for each session, making it difficult to ascertain whether such users participate more than once. Tag Along with Adler had 3,557 registered volunteers, with an additional 3,419 single-use identification sessions, for a maximum total of 6,976 individual users.[/ref] Of these participants, one noticeable subset stood out: the superuser. A known entity in crowdsourcing projects, superusers are a small number of users who contribute a large percentage of the activity (in contrast to the larger number of users who make fewer contributions in total).[ref]Frauke Rohden et al., \u201cTagging, Pinging and Linking\u2014User Roles in Virtual Citizen Science Forums,\u201d Citizen Science: Theory and Practice 4, no. 1 (June 7, 2019): 19, https://doi.org/10.5334/cstp.181 .[/ref] In one review of more than sixty online citizen-science projects, researchers confirmed that the presence of superusers can be at odds with project models meant to capture a diversity of perspectives.[ref] Spiers et al., \u201cEveryone Counts?\u201d[/ref] Though engaging a community of dedicated and experienced volunteers who consistently return to a project enables quicker and more accurate data processing, metadata projects that are specifically not looking for a consensus or single accurate answer must be implemented in ways that encourage the participation of a broad range of users alongside a dedicated base of superusers. Specific ways Tag Along with Adler aimed to accomplish this included requiring fifty people to classify an image before it was retired from the project, and releasing small amounts of data incrementally (eleven sets of one hundred images). All of these design decisions helped prevent superusers from blowing through the project data quickly and thereby limiting the diversity of user approaches.[ref]Simon Fuger at al., \u201cUser Roles and Team Structures in a Crowdsourcing Community for International Development\u2014A Social Network Perspective,\u201d Information Technology for Development 23, no. 3 (July 2017): 438\u201362, https://doi.org/10.1080/02681102.2017.1353947 . See also Lesandro Ponciano and Francisco Brasileiro, \u201cFinding Volunteers\u2019 Engagement Profiles in Human Computation for Citizen Science Projects,\u201d Human Computation 1, no. 2 (December 2014), https://doi.org/10.15346/hc.v1i2.12 ; and \u201cProject Update: Gamifying the Transcription of Bentham\u2019s Writings,\u201d Transcribe Bentham, University College London, February 2019, https://blogs.ucl.ac.uk/transcribe-bentham/category/events/ .[/ref] Initial processing of Tag Along with Adler \u2019s data confirmed the presence of superusers, although it appears that the design choices just mentioned were ultimately effective in encouraging the participation of both superusers and a larger group of users. Of the 3,557 registered volunteers who have participated in Tag Along with Adler at the time of writing, only 807 users participated in more than one subject set (that is, returned to the project for additional releases of more images). Across the eleven subject sets, these 3,557 registered volunteers created 322,993 individual metadata tags. Those who participated in only one subject set (that is, who did not return to the project over time) submitted 163,342 of these tags (50.6%), averaging 59 tags per user. In comparison, those who worked on multiple subject sets (that is, who returned to the project more than once over the course of its twelve-month duration) submitted 159,651 of the tags (49.4%), averaging 197.8 tags per user over the course of their participation. These results reflect previous analyses of superusers, including a 2019 study that looked at the virtual citizen-science project Shakespeare\u2019s World (also on Zooniverse), finding that 37% of the total content was created by only 3% of the project participants.[ref]Rohden et al., \u201cTagging, Pinging and Linking,\u201d 19.[/ref] Tag Along with Adler results show that roughly 22% of the project participants are responsible for the creation of 49.4% of the project\u2019s data. Figure 1 demonstrates the number of users returning for each subject set and the median number of tags created per user, per set. The inverse relationship demonstrates that although the rate of returning users generally falls off with each set, the tags created by each user continues to go up as they return to the project (showing that those who return to the project create more tags per image for each set they return to, than those who participate only once), attesting to the involvement of a dedicated base of engaged superusers and also a large group of users who together generate the bulk of the tags.   Several considerations motivated the choice to center this research within the collections of the Adler, to use the preexisting third-party platform Zooniverse, and to test iteratively across various workflows and projects. However, each of these choices comes with limitations that must be acknowledged to accurately assess the potential of such projects outside of the Adler and across the cultural heritage sector more broadly. We immediately recognized the limitation of this case study\u2019s reliance on English at the expense of other languages. Over two-thirds of Zooniverse\u2019s users identify as residents of the United States or the United Kingdom, and the majority of the public projects on the site are only available in English.[ref] Robert Simpson, \u201cWho are the Zooniverse Community? We Asked Them . . . ,\u201d Zooniverse (blog), Zooniverse, March 5, 2015, https://blog.zooniverse.org/2015/03/05/who-are-the-zooniverse-community-we-asked-them .[/ref] The Adler does not have demographic information on the languages our guests use, but it is worth noting that 35.8% of Chicago residents speak a language other than English.[ref] United States Census Bureau, \u201cNon-English Speakers: Most Common Languages, Chicago, IL,\u201d Accessed August 20, 2021, https://data.census.gov/table?q=languages+in+Chicago+&tid=ACSST1Y2021.S1601 .[/ref] Although these kinds of metrics do not provide a clear idea of how many non\u2013English speakers are precluded from participating, as residents who speak a language other than English may also speak English, they do remind us that engaging publics in only one language will inevitably lead to some degree of exclusion.[ref]Conversations are still ongoing at the Adler about the possibilities of incorporating catalogue and metadata tags in non-English languages.[/ref] Other types of diversity among project participants are also a critical marker of representational equity. Because an explicit purpose of our team\u2019s crowdsourcing project was to enhance and expand the accessibility and representation of catalogue data, ensuring the involvement of a representative public was vital. The Tag Along with Adler header on the project\u2019s Zooniverse landing page collected demographic information through a voluntary survey.[ref] The survey that accompanies the project can be accessed here: https://forms.gle/JZ3fuZhKdvahe5dm7 .[/ref] We made the survey voluntary as we recognized that it would be unethical to require demographic information as a condition of participation, as such a requirement could be a barrier to entry for those uncomfortable identifying various aspects of their identity, especially to an institution by which they may feel tokenized or othered, or from which they may feel excluded. At the time of writing, 107 of the project\u2019s 3,557 registered users (or roughly 5.5%) have participated in this survey. The data presented here breaks down responses to questions centered on education level (see fig. 2), ethnicity (see fig. 3), gender (see fig. 4), and age (see fig. 5).[ref]Spiers et al. identifies additional user demographic statistics for age and gender across five Zooniverse projects, although no information was recorded for race or education level.[/ref] These results show a strong alignment with those of previous surveys on traditional crowdsourcing platforms. Most notably, just shy of two-thirds of respondents self-identified as white/Caucasian, and ethnic diversity overall was the least distributed of the four demographics gauged. A 2020 study published in the journal Citizen Science: Theory and Practice found that data from the surveys of an Illinois citizen-science project called RiverWatch indicated that participants were \u201cdisproportionately white, highly educated, and affluent compared with the Illinois general population.\u201d[ref] Charlie Blake, Allison Rhanor, and Cody Pajic, \u201cThe Demographics of Citizen Science Participation and Its Implications for Data Quality and Environmental Justice,\u201d Citizen Science: Theory and Practice 5, no. 1 (October 2020): 21, https://doi.org/10.5334/cstp.320 .[/ref] Education levels among those surveyed for Tag Along with Adler varied significantly more than racial identity, however, in part due to the active participation of many high school students, and the percentage of participants with bachelor\u2019s degrees (28.7%) closely matched the percentage of Americans with bachelor\u2019s degrees as recorded by the US Census between 2015 and 2019 (32.1%).[ref] Michael T. Nietzel, \u201cNew from U.S. Census Bureau: Number of Americans with a Bachelor\u2019s Degree Continues to Grow,\u201d Forbes , Feburary 22, 2021, https://www.forbes.com/sites/michaeltnietzel/2021/02/22/new-from-us-census-bureau-number-of-americans-with-a-bachelors-degree-continues-to-grow/?sh=106c61957bbc .[/ref] Participants also logged a diverse array of ages in their survey responses, with the largest proportion coming from school-aged students eighteen or under and a decent distribution among the remaining age groups. This data comes with limitations, clearly\u2014as already stated, only 5.5% of users thus far have opted in to provide it\u2014but it does help provide a basis for claiming that the project data was produced by a group more representative of the general public than is the predominantly white and female professional museum staff who otherwise would have catalogued these images.[ref]In 2018 the Andrew W. Mellon Foundation, the Association of Art Museum Directors, and the American Alliance of Museums published a report on the demographics of current museum staff. The report indicates that, in art museums, staff are predominately female, though most senior leadership positions are held by men. Staff are also predominantly white (72%), with leadership positions approximately 80% white. The reports did not study age or educational level of museum staff. See Mari\u00ebt Westermann, Liam Sweeney, and Roger Schonfeld, \u201cArt Museum Staff Demographic Survey 2018,\u201d Ithaka S+R, January 28, 2019, https://doi.org/10.18665/sr.310935 .[/ref]   In addition to providing the quantitative data discussed above, the optional survey also provided the project team with qualitative baselines on the engagement of users with the project itself and with its overall learning objectives. This qualitative data\u2014along with that collected from other components like the online platform\u2019s discussion threads\u2014is also essential for discussing crowdsourcing projects as the kind of mission-centric, participatory experience that this paper asserts they are. One set of survey questions was particularly helpful in gauging audience perspectives on concepts like trust and representation with the publics to whom we are reaching out. Figure 6 graphs the responses to eight survey questions about museums, science, communities, and representation. Response options ranged from Strongly Disagree to Strongly Agree. Even with only 5.5% of users reporting, the responses support the project team\u2019s initial hypotheses about the value of museum crowdsourcing projects. Approximately 23.9% of survey respondents did not agree with the statement \u201cStories like mine are in museum collections,\u201d 69% did not agree with the statement \u201cStories like mine are included in museum exhibitions,\u201d and 39.6% did not agree with the statement that \u201cI see people like me in science today.\u201d The extremely high percentage of participants who felt museums were essential to communities (94.4%) and communities were essential to museums (94.9%) points to a clear opportunity for museums to leverage their position within the community to initiate participatory experiences that bring the public into the process of description, helping to not only increase the representation that is notably lacking in professional cataloguing staff but also transparently bringing the community into the essential work of the museum.   There is also an opportunity for museums\u2014as institutions with recognized standing in the community\u2014to help foster discussions about searchability and discovery on the internet. Our survey results show that only 18.7% of participants agreed they could trust what they find online but, conversely, 76% of them believe they can find things online easily. This study\u2019s degree of transparency\u2014and that of crowdsourcing projects in general\u2014could be adapted to empower communities to better distinguish between fact and fiction online and to increase their trust of online searches by imparting knowledge of how to identify bias and recognize shortcomings in automation and algorithms, including but hardly limited to searches of a museum\u2019s collection. The data thus far has illuminated user behavior and the limitations typical of any crowdsourcing project, from the presence of superusers to the demographics of participants. However, it is also possible to use this data to demonstrate both that the semantic gap between the language and description style museum professionals use (e.g., technical language, focus on physicality and provenance) and the language and description style the public uses (e.g., conversational language, focus on context and aboutness) does in fact exist and that projects like these can begin to bridge this gap. As discussed above, the Adler project team conducted a full survey of the Adler cataloguing data in conjunction with the design of the project, and the most frequent terms across extant Adler records (see fig. 7) were locations of objects (where objects were created, where books were published), item types (instrument names, book types [folios, manuscripts], document types), date of creation, and creators (object makers, authors, etc.). Although this data is clearly important for recording the provenance and overall historicity of the objects, it does not contribute significantly to an understanding of their aboutness.   By comparison, the participants working with the eleven subject sets of Tag Along with Adler eschewed terms focusing on the \u201cisness\u201d of makers, locations, and dates, instead producing language geared toward describing what is represented in an object (although importantly still including terms related to object type such as \u201cdiagram,\u201d \u201cdrawing,\u201d and \u201cphotograph\u201d) (see fig. 8). Comparing even only the thirty most frequent terms from the Adler catalogue and the thirty most frequent tags from the Tag Along with Adler project reveals a distinct gap between the way museums and the public describe collections. These results help to show that crowdsourcing does have the desired effect of enhancing collections records to better suit the language of their users, which will go a long way toward improving the searchability of collections, especially for the public.[ref]Work continues at the Adler on a process for maintaining quality control in user-generated tags. Accuracy is extremely important\u2014looking at spelling, facticity, polysemy, and plurality, for example\u2014but as this is ongoing work, this essay will not expand on it but only mention here that it is a necessary step to either be designed into the project for future users to assist with, or assigned as part of staff workload.[/ref]   Figure 9 visualizes the importance of adding user language to institutions\u2019 digital catalogues. Next to the image, two columns of text show the terms that were present in the Adler catalogue for this image before the Tag Along with Adler project (left) and the tags generated by users during the project (right). This comparison lays bare not only the difference in how museum cataloguers and general audiences describe images but also the sheer number of access points that can be added by a crowdsourcing project, again increasing discoverability of a collection\u2019s contents for staff and public alike. These expanded access points will allow internal staff to query the database of images for specific subjects, in order to select objects for virtual exhibitions, public programming, or social media. As collaborative hashtags spread across museum social media such as #MuseumSunshine, having discoverable language focused on the context and pictorial dimension of object\u2019s (i.e., their aboutness) helps aid staff members in finding appropriate collections to include, and also helps share collections within wider hashtags and movements that may expand the reach of the institution to new publics.[ref]On #MuseumSunshine, see \u201cClosed Art Institutions Brighten Up the Day With #MuseumSunshine Images,\u201d Observer, April 22, 2020, https://observer.com/2020/04/museum-sunshine-art-institutions-twitter-hashtag/ . On museums and hashtags more generally, see Brendan Ciecko, Hilary-Morgan Watt, and Emily Haight, \u201cHow Museums Can Experiment with Social Media to Boost Audience Engagement During Coronavirus,\u201d Cuseum, April 1, 2020, https://cuseum.com/webinars/how-museums-can-experiment-with-social-media-to-boost-audience-engagement-during-coronavirus-overview .[/ref]   Finally, when processing the data for the Tag Along with Adler project, we counted how many tags participants added and analyzed their overlap with, on the one hand, tags already in the Adler catalogue and, on the other, tags created by AI models. For this project, only 12.2% of user-generated tags were already in the Adler catalogue, meaning 87.8% of their tags were new, and only about 7.25% of the tags users generated were also created by the AI models, meaning 92.75% of the tags users created were not generated by AI. This was an exciting early assurance of the importance of using crowdsourcing for metadata creation, as it shows that although AI has some promise for metadata and tag creation, its success is still extremely dependent on the dataset on which the AI model was trained and is still far inferior to the work of human participants, at least in terms of describing varied materials from multiple points of view. For example, AI is often unable to pick up nuance or situational knowledge that humans can; for archival photographs at the Adler Planetarium, the AI inaccurately tagged scientists in lab coats as doctors or nurses. Human volunteers added tags for \u201cscientists,\u201d \u201clabcoats,\u201d and \u201clab coats,\u201d but did not include tags for \u201cdoctor\u201d or \u201cnurse\u201d as the image did not show medical equipment or hospital backgrounds that would prompt such tags. However, the inclusion of AI tags in this project clearly enticed and motivated some volunteers\u2019 participation: the \u201cVerify AI Tags\u201d workflow consistently saw two to three times more engagement than that of the \u201cTag Images\u201d workflow, demonstrating the draw that AI, automation, and algorithms can offer. A Case for Crowdsourcing Crowdsourcing offers the opportunity for museums to leverage a novel methodology to build new relationships with their audiences, disrupting the usual relationship between the museum and the user by inviting the public to act as curators, experts, and researchers. In the process, it simultaneously enriches the user\u2019s experience and the museum\u2019s data and access points.[ref]Mia Ridge et al., \u201cIntroduction and Colophon,\u201d The Collective Wisdom Handbook: Perspectives on Crowdsourcing in Cultural Heritage\u2014Community Review Version, April 29, 2021, https://britishlibrary.pubpub.org/pub/introduction-and-colophon .[/ref] Crowdsourcing also ultimately expands the museum\u2019s voice by incorporating a vocabulary and style of description aligned with the public\u2019s own intellectual interests and perceptions. It thus expands who can access these collections while also allowing for mission-driven experiences that encourage engagement with the institution. If we as museum professionals acknowledge that in the contemporary online ecosystem, the public contends with misinformation, inherent biases in the results of their searches, and frequently invisible AI that underpins their methods of discovery, then now is not the time to uphold a status quo that hinders the very missions of our institutions by hampering the ability to discover our collections. In the words of the Collective Wisdom project, \u201cdoing nothing is also a decision. Doing nothing in this context, by choosing not to engage with values, is likely to support the status quo, including existing power structures, instead of taking the opportunity for challenge and consciously course-setting.\u201d[ref]Ibid.[/ref] It is time to examine and consider projects like crowdsourcing as an extension of museums\u2019 mission-driven work and to see the value of including the voices of the communities we serve in the work that we present and the projects we initiate. By considering such participation part of the mission-centric work of the institution, it is possible to devote the staff, time, and resources needed to address contemporary discussions about the issues affecting the lives of the public both inside and outside of museums.[ref]Ibid.[/ref] The results of Tag Along with Adler not only further our understanding of a research topic situated within decades-long literature reviews but also provides a specific and concrete case study evolving in real time through the actual work undertaken at the Adler Planetarium. As the public\u2019s online habits and access to collections have shifted, it has become more important for both museums and the public that cataloguing focus as much on isness as aboutness. This essay bolsters the call for more institutions to engage in crowdsourcing and metadata-generating projects that utilize the enthusiasm and insight of their audiences. These projects create meaningful opportunities for the public to experience collections while making them more easily discoverable by others. Increasing transparency, accessibility, and representation within collections is a central component of museums\u2019 online presences, and crowdsourcing is proving to be one of the most effective ways to do this work.   Banner image: Shoolchildren watching a demonstration at the Adler Planetarium, Chicago, mid-twentieth century. Courtesy of the Adler Planetarium Collections, APHP.S2.F18.1\" ",
    "author_display": "Jessica BrodeFrank",
    "digital_publication_id": 37,
    "source_updated_at": null,
    "updated_at": "2023-09-23T23:48:08-05:00",
    "timestamp": "2023-09-24T02:13:14-05:00"
}